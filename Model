from google.colab import drive
drive.mount('/content/drive')

!pip install torch==2.4.0 torchvision==0.19.0
!pip install mamba-ssm==2.2.4 --no-build-isolation

''' Torch '''
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.utils.data import Dataset, DataLoader

''' Mamba '''
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
from functools import partial
from typing import Optional, Callable
from timm.models.layers import DropPath
import math

''' Scikit '''
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils import shuffle
from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix

''' Scipy '''
import scipy.io
import scipy.io as sio
from scipy.ndimage import gaussian_filter
from scipy.io import loadmat

''' Einops '''
from einops import rearrange, repeat
from einops import reduce

''' Other '''
from random import randint
import time
import matplotlib.pyplot as plt
import numpy as np
from IPython.display import clear_output
import pandas as pd
import os,re
import errno
import random
import urllib.request as urllib
import librosa
import numpy as np
import cv2
import functools
from tqdm import tqdm
import seaborn as sns

from einops import rearrange

class PCA(nn.Module):
  def __init__(self, dim,kernel_size):
    super().__init__()
    self.dw = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding="same", groups=dim)
    self.prob = nn.Softmax(dim=1)

  def forward(self,x):
    c = reduce(x, 'b c h w -> b c', 'mean')
    x = self.dw(x)
    c_ = reduce(x, 'b c h w -> b c', 'mean')
    raise_ch = self.prob(c_ - c)
    att_score = torch.sigmoid(c_ + c_*raise_ch)
    return torch.einsum('bchw, bc -> bchw', x, att_score)

class PSA(nn.Module):
  def __init__(self, dim,kernel_size):
    super().__init__()
    self.pw = nn.Conv2d(dim, dim, kernel_size=1)
    self.prob = nn.Softmax2d()

  def forward(self,x):
    s = reduce(x, 'b c w h -> b w h', 'mean')
    xp = self.pw(x)
    s_ = reduce(xp, 'b c w h -> b w h', 'mean')
    raise_sp = self.prob(s_ - s)
    att_score = torch.sigmoid(s_ + s_*raise_sp)
    return torch.einsum('bchw, bwh -> bchw', x, att_score)

class LKA(nn.Module):
    def __init__(self, dim, kernel_size, dilated_rate=3):
        super().__init__()
        #self.conv0 = nn.Conv2d(dim, dim, kernel_size, padding='same', groups=dim)
        self.conv0 = PCA(dim,kernel_size=kernel_size)
        self.conv_spatial = nn.Conv2d(dim, dim, kernel_size=7, stride=1, padding='same', groups=dim, dilation=dilated_rate)
        #self.conv1 = nn.Conv2d(dim, dim, 1)
        self.conv1 = PSA(dim,kernel_size=kernel_size)
        self.norm = nn.BatchNorm2d(dim)
    def forward(self, x):
        u = x.clone()
        attn = self.conv0(x)
        attn = self.conv_spatial(attn)
        # attn = F.sigmoid(self.conv1(attn))
        return u*attn

class my_norm(nn.Module):
    def __init__(self, shape=4096):
        super().__init__()
        self.shape = shape
        self.norm = nn.LayerNorm(shape)
    def forward(self, x):
        B,C,H,W = x.shape
        x = x.view(B,C,-1)
        x = self.norm(x)
        x = x.view(B,C,H,W)
        return x

class MultiScaleExtractor(nn.Module):
    def __init__(self, dim=64):
        super().__init__()
        # self.head_pw = nn.Conv2d(dim, dim, 1)
        self.tail_pw = nn.Conv2d(dim, dim, 1)

        self.LKA3 = LKA(dim, kernel_size=3)
        self.LKA5 = LKA(dim, kernel_size=5)
        self.LKA7 = LKA(dim, kernel_size=7)
        self.norm3 = nn.BatchNorm2d(dim)
        self.norm5 = nn.BatchNorm2d(dim)
        self.norm7 = nn.BatchNorm2d(dim)

        self.pointwise = nn.Conv2d(dim, dim, 1)
        self.conv_cn = nn.Conv2d(dim, dim, 3, groups=dim,padding=1)
        self.norm_last = nn.BatchNorm2d(dim)
    def forward(self, x):
        x_copy = x.clone()
        # x = self.head_pw(x)

        x3 = self.LKA3(x) + x
        x3 = self.norm3(x3)
        x5 = self.LKA5(x) + x
        x5 = self.norm5(x5)
        x7 = self.LKA7(x) + x
        x7 = self.norm7(x7)

        x = F.gelu(x3 + x5 + x7)
        x = self.tail_pw(x) + x_copy

        x = self.pointwise(x)
        x = self.conv_cn(x)
        x = F.gelu(self.norm_last(x))
        return x

def Feature_Extractor(dim=64, patch_size=4, depth=2):
    return nn.Sequential(
      *[MultiScaleExtractor(dim=dim) for _ in range(depth)]
    )

import time
import math
from functools import partial
from typing import Optional, Callable
from torch import Tensor
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from einops import rearrange, repeat
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
try:
    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
except:
    pass

# an alternative for mamba_ssm (in which causal_conv1d is needed)
try:
    from selective_scan import selective_scan_fn as selective_scan_fn_v1
    from selective_scan import selective_scan_ref as selective_scan_ref_v1
except:
    pass

DropPath.__repr__ = lambda self: f"timm.DropPath({self.drop_prob})"


def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):
    """
    u: r(B D L)
    delta: r(B D L)
    A: r(D N)
    B: r(B N L)
    C: r(B N L)
    D: r(D)
    z: r(B D L)
    delta_bias: r(D), fp32

    ignores:
        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]
    """
    import numpy as np

    # fvcore.nn.jit_handles
    def get_flops_einsum(input_shapes, equation):
        np_arrs = [np.zeros(s) for s in input_shapes]
        optim = np.einsum_path(equation, *np_arrs, optimize="optimal")[1]
        for line in optim.split("\n"):
            if "optimized flop" in line.lower():
                # divided by 2 because we count MAC (multiply-add counted as one flop)
                flop = float(np.floor(float(line.split(":")[-1]) / 2))
                return flop


    assert not with_complex

    flops = 0 # below code flops = 0
    if False:
        ...
        """
        dtype_in = u.dtype
        u = u.float()
        delta = delta.float()
        if delta_bias is not None:
            delta = delta + delta_bias[..., None].float()
        if delta_softplus:
            delta = F.softplus(delta)
        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]
        is_variable_B = B.dim() >= 3
        is_variable_C = C.dim() >= 3
        if A.is_complex():
            if is_variable_B:
                B = torch.view_as_complex(rearrange(B.float(), "... (L two) -> ... L two", two=2))
            if is_variable_C:
                C = torch.view_as_complex(rearrange(C.float(), "... (L two) -> ... L two", two=2))
        else:
            B = B.float()
            C = C.float()
        x = A.new_zeros((batch, dim, dstate))
        ys = []
        """

    flops += get_flops_einsum([[B, D, L], [D, N]], "bdl,dn->bdln")
    if with_Group:
        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], "bdl,bnl,bdl->bdln")
    else:
        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], "bdl,bdnl,bdl->bdln")
    if False:
        ...
        """
        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))
        if not is_variable_B:
            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)
        else:
            if B.dim() == 3:
                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)
            else:
                B = repeat(B, "B G N L -> B (G H) N L", H=dim // B.shape[1])
                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)
        if is_variable_C and C.dim() == 4:
            C = repeat(C, "B G N L -> B (G H) N L", H=dim // C.shape[1])
        last_state = None
        """

    in_for_flops = B * D * N
    if with_Group:
        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], "bdn,bdn->bd")
    else:
        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], "bdn,bn->bd")
    flops += L * in_for_flops
    if False:
        ...
        """
        for i in range(u.shape[2]):
            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
            if not is_variable_C:
                y = torch.einsum('bdn,dn->bd', x, C)
            else:
                if C.dim() == 3:
                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])
                else:
                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])
            if i == u.shape[2] - 1:
                last_state = x
            if y.is_complex():
                y = y.real * 2
            ys.append(y)
        y = torch.stack(ys, dim=2) # (batch dim L)
        """

    if with_D:
        flops += B * D * L
    if with_Z:
        flops += B * D * L
    if False:
        ...
        """
        out = y if D is None else y + u * rearrange(D, "d -> d 1")
        if z is not None:
            out = out * F.silu(z)
        out = out.to(dtype=dtype_in)
        """

    return flops


class PatchEmbed2D(nn.Module):
    r""" Image to Patch Embedding
    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """
    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):
        super().__init__()
        if isinstance(patch_size, int):
            patch_size = (patch_size, patch_size)
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        x = self.proj(x).permute(0, 2, 3, 1)
        if self.norm is not None:
            x = self.norm(x)
        return x


class PatchMerging2D(nn.Module):
    r""" Patch Merging Layer.
    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        B, H, W, C = x.shape

        SHAPE_FIX = [-1, -1]
        if (W % 2 != 0) or (H % 2 != 0):
            print(f"Warning, x.shape {x.shape} is not match even ===========", flush=True)
            SHAPE_FIX[0] = H // 2
            SHAPE_FIX[1] = W // 2

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C

        if SHAPE_FIX[0] > 0:
            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]
            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]
            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]
            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]

        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x


class PatchExpand2D(nn.Module):
    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim*2
        self.dim_scale = dim_scale
        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)
        self.norm = norm_layer(self.dim // dim_scale)

    def forward(self, x):
        B, H, W, C = x.shape
        x = self.expand(x)

        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)
        x= self.norm(x)

        return x


class Final_PatchExpand2D(nn.Module):
    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.dim_scale = dim_scale
        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)
        self.norm = norm_layer(self.dim // dim_scale)

    def forward(self, x):
        B, H, W, C = x.shape
        x = self.expand(x)

        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)
        x= self.norm(x)

        return x


class SS2D(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        # d_state="auto", # 20240109
        d_conv=3,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        dropout=0.,
        conv_bias=True,
        bias=False,
        device=None,
        dtype=None,
        **kwargs,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        # self.d_state = math.ceil(self.d_model / 6) if d_state == "auto" else d_model # 20240109
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank

        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)
        self.conv2d = nn.Conv2d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            groups=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            padding=(d_conv - 1) // 2,
            **factory_kwargs,
        )
        self.act = nn.SiLU()

        self.x_proj = (
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),
        )
        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)
        del self.x_proj

        self.dt_projs = (
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
        )
        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)
        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)
        del self.dt_projs

        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)
        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)

        # self.selective_scan = selective_scan_fn
        self.forward_core = self.forward_corev0

        self.out_norm = nn.LayerNorm(self.d_inner)
        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)
        self.dropout = nn.Dropout(dropout) if dropout > 0. else None

    @staticmethod
    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):
        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(dt_proj.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            dt_proj.bias.copy_(inv_dt)
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        dt_proj.bias._no_reinit = True

        return dt_proj

    @staticmethod
    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):
        # S4D real initialization
        A = repeat(
            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        if copies > 1:
            A_log = repeat(A_log, "d n -> r d n", r=copies)
            if merge:
                A_log = A_log.flatten(0, 1)
        A_log = nn.Parameter(A_log)
        A_log._no_weight_decay = True
        return A_log

    @staticmethod
    def D_init(d_inner, copies=1, device=None, merge=True):
        # D "skip" parameter
        D = torch.ones(d_inner, device=device)
        if copies > 1:
            D = repeat(D, "n1 -> r n1", r=copies)
            if merge:
                D = D.flatten(0, 1)
        D = nn.Parameter(D)  # Keep in fp32
        D._no_weight_decay = True
        return D

    def forward_corev0(self, x: torch.Tensor):
        self.selective_scan = selective_scan_fn

        B, C, H, W = x.shape
        L = H * W
        K = 4

        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)
        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)

        x_dbl = torch.einsum("b k d l, k c d -> b k c l", xs.view(B, K, -1, L), self.x_proj_weight)
        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)
        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)
        dts = torch.einsum("b k r l, k d r -> b k d l", dts.view(B, K, -1, L), self.dt_projs_weight)
        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)

        xs = xs.float().view(B, -1, L) # (b, k * d, l)
        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)
        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Ds = self.Ds.float().view(-1) # (k * d)
        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)
        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)

        out_y = self.selective_scan(
            xs, dts,
            As, Bs, Cs, Ds, z=None,
            delta_bias=dt_projs_bias,
            delta_softplus=True,
            return_last_state=False,
        ).view(B, K, -1, L)
        assert out_y.dtype == torch.float

        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)
        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)

        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y

    # an alternative to forward_corev1
    def forward_corev1(self, x: torch.Tensor):
        self.selective_scan = selective_scan_fn_v1

        B, C, H, W = x.shape
        L = H * W
        K = 4

        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)
        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)

        x_dbl = torch.einsum("b k d l, k c d -> b k c l", xs.view(B, K, -1, L), self.x_proj_weight)
        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)
        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)
        dts = torch.einsum("b k r l, k d r -> b k d l", dts.view(B, K, -1, L), self.dt_projs_weight)
        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)

        xs = xs.float().view(B, -1, L) # (b, k * d, l)
        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)
        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Ds = self.Ds.float().view(-1) # (k * d)
        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)
        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)

        out_y = self.selective_scan(
            xs, dts,
            As, Bs, Cs, Ds,
            delta_bias=dt_projs_bias,
            delta_softplus=True,
        ).view(B, K, -1, L)
        assert out_y.dtype == torch.float

        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)
        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)

        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y

    def forward(self, x: torch.Tensor, **kwargs):
        B, H, W, C = x.shape

        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)

        x = x.permute(0, 3, 1, 2).contiguous()
        x = self.act(self.conv2d(x)) # (b, d, h, w)
        y1, y2, y3, y4 = self.forward_core(x)
        assert y1.dtype == torch.float32
        y = y1 + y2 + y3 + y4
        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)
        y = self.out_norm(y)
        y = y * F.silu(z)
        out = self.out_proj(y)
        if self.dropout is not None:
            out = self.dropout(out)
        return out


def channel_shuffle(x: Tensor, groups: int) -> Tensor:

    batch_size, height, width, num_channels = x.size()
    channels_per_group = num_channels // groups

    # reshape
    # [batch_size, num_channels, height, width] -> [batch_size, groups, channels_per_group, height, width]
    x = x.view(batch_size, height, width, groups, channels_per_group)

    x = torch.transpose(x, 3, 4).contiguous()

    # flatten
    x = x.view(batch_size, height, width, -1)

    return x

class SS_Conv_SSM(nn.Module):
    def __init__(
        self,
        hidden_dim: int = 0,
        drop_path: float = 0,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
        attn_drop_rate: float = 0,
        d_state: int = 16,
        **kwargs,
    ):
        super().__init__()
        self.ln_1 = norm_layer(hidden_dim//2)
        self.self_attention = SS2D(d_model=hidden_dim//2, dropout=attn_drop_rate, d_state=d_state, **kwargs)
        self.drop_path = DropPath(drop_path)

        self.Feature_Extractor = Feature_Extractor(32)
        # self.finalconv11 = nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, stride=1)
    def forward(self, input: torch.Tensor):
        ori=input

        input_left, input_right = input.chunk(2,dim=1)
        input_right = self.drop_path(self.self_attention(self.ln_1(input_right.permute(0,2,3,1))))
        input_right = input_right.permute(0,3,1,2).contiguous()

        input_left = self.Feature_Extractor ( input_left)

        output = torch.cat((input_left,input_right),dim=1)
        output = channel_shuffle(output,groups=2)
        return output+ori

class Encoder(nn.Module):
    def __init__(self, dim):
        super(Encoder, self).__init__()
        self.dim = dim
        self.GAP = nn.AdaptiveAvgPool2d((1, 1))
        self.ScaledDotProductAttention = ScaledDotProductAttention()
        self.norm = nn.LayerNorm(normalized_shape=self.dim)
        self.FFN = nn.Sequential(
            nn.Linear(self.dim, self.dim),
            nn.GELU(),
            nn.Linear(self.dim, self.dim),
            nn.GELU()
        )

    def forward(self, Support):
      encoded = []
      for index in range(len(Support)):
          s = Support[index]                                  # [B, C, H, W]
          s = self.GAP(s)                                     # [B, C, 1, 1]
          s = s.view(s.size(0), s.size(1))                    # [B, C]
          s = self.ScaledDotProductAttention(s, s, s) + s     # [B, C]
          s = self.norm(s)                                    # [B, C]
          s = self.FFN(s) + s                                 # [B, C]
          s = self.norm(s)                                    # [B, C]
          s = torch.mean(s, dim=0, keepdim=True)
          encoded.append(s)

      return encoded                                         # [Num_class x (B, C)]

class ScaledDotProductAttention(nn.Module):
    def __init__(self, dim=64):
        super(ScaledDotProductAttention, self).__init__()
        self.q_linear = nn.Linear(dim, dim)
        self.k_linear = nn.Linear(dim, dim)
        self.v_linear = nn.Linear(dim, dim)
        self.dim = dim

    def forward(self, q, k, v):
        """
        Args:
            q (Tensor): Query tensor of shape (batch_size, dim).
            k (Tensor): Key tensor of shape (batch_size, dim).
            v (Tensor): Value tensor of shape (batch_size, dim).

        Returns:
            output (Tensor): Scaled Dot-Product Attention output tensor of shape (batch_size, dim).
        """
        q = self.q_linear(q)
        k = self.k_linear(k)
        v = self.v_linear(v)

        scaled_dot_product = torch.matmul(q.unsqueeze(2), k.unsqueeze(1)) / torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))
        attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)
        output = torch.matmul(attention_weights, v.unsqueeze(2))
        output = output.squeeze(2)
        return output

class CrossAttention(nn.Module):
    def __init__(self, dim=64):
        super(CrossAttention, self).__init__()
        self.q_linear = nn.Linear(dim, dim)
        self.k_linear = nn.Linear(dim, dim)
        self.v_linear = nn.Linear(dim, dim)
        self.dim = dim
    def forward(self, q, k, v):
        """
        Args:
            q (Tensor): Query tensor of shape (batch_size,  dim).
            k (Tensor): Key tensor of shape (batch_size, dim).
            v (Tensor): Value tensor of shape (batch_size, dim).

        Returns:
            output (Tensor): Scaled Dot-Product Attention output tensor of shape (batch_size, dim).
        """
        q = self.q_linear(q)
        k = self.k_linear(k)



        v = self.v_linear(v)

        scaled_dot_product = torch.matmul(q.unsqueeze(2), k.unsqueeze(1))
        attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)
        # output = torch.matmul(attention_weights, v)

        return attention_weights

class Encoder_Decoder(nn.Module):
    def __init__(self, dim):
        super(Encoder_Decoder, self).__init__()
        self.dim = dim
        self.encoder_out = Encoder(self.dim)
        self.attention = CrossAttention()
        self.ScaledDotProductAttention = ScaledDotProductAttention()
        self.norm = nn.LayerNorm(normalized_shape=self.dim)
        self.GAP = nn.AdaptiveAvgPool2d((1, 1))
        self.Linear = nn.Conv1d(in_channels=dim,out_channels=dim,kernel_size=dim)
    def forward(self, q, S):
        q = self.GAP(q)                                       # [B, C, 1, 1]
        q = q.view(q.size(0), q.size(1))                      # [B, C]
        q_first = q                                           # [B, C]
        q = self.ScaledDotProductAttention(q, q, q) + q_first # [B, C]
        q = self.norm(q)                                      # [B, C]
        output = []
        encoder_outs = self.encoder_out(S)                    # [Num_class x (B, C)]

        for encoder_out in encoder_outs:
            out = self.attention(q, encoder_out, encoder_out) # [B, C, C]
            out = self.Linear(out)                            # [B,C,1]
            out = out.squeeze(2)                              # [B,C]
            output.append(out)
        output=torch.stack(output,dim=1)


        return output                                         # [B,Num_class, C)]
import torch
import torch.nn as nn
import torch.nn.functional as F
import functools


class MahalanobisBlock(nn.Module):
    def __init__(self,B,C,H,W):
        super(MahalanobisBlock, self).__init__()
        self.B=B
        self.C=C
        self.H=H
        self.W=W
        self.conv1d=nn.Conv1d(self.B,self.B,self.H*self.W,self.H*self.W)

    def cal_covariance(self, input):
        CovaMatrix_list = []
        for i in range(len(input)):
            support_set_sam = input[i]
            B, C, h, w = support_set_sam.size()
            local_feature_list = []

            for local_feature in support_set_sam:
                local_feature_np = local_feature.detach().cpu().numpy()
                transposed_tensor = np.transpose(local_feature_np, (1, 2, 0))
                reshaped_tensor = np.reshape(transposed_tensor, (h * w, C))

                for line in reshaped_tensor:
                    local_feature_list.append(line)

            local_feature_np = np.array(local_feature_list)
            # mean = np.mean(local_feature_np, axis=0)
            # local_feature_list = [x - mean for x in local_feature_list]

            covariance_matrix = np.cov(local_feature_np, rowvar=False)
            covariance_matrix = torch.from_numpy(covariance_matrix)
            CovaMatrix_list.append(covariance_matrix)

        return CovaMatrix_list # [numclass*[C,C]]



    def mahalanobis_similarity(self, input, CovaMatrix_list):
        B, C, h, w = input.size()
        mahalanobis = []
        in_channels=B
        out_channels=B
        kernel_size=h*w
        stride=h*w

        for i in range(B):
            query_sam = input[i]
            query_sam = query_sam.view(C, -1)
            query_sam_norm = torch.norm(query_sam, 2, 1, True)
            query_sam = query_sam / query_sam_norm
            mea_sim = torch.zeros(1, len(CovaMatrix_list) * h * w).cuda()
            for j in range(len(CovaMatrix_list)):
                covariance_matrix = CovaMatrix_list[j].float().cuda()
                diff = query_sam - torch.mean(query_sam, dim=1, keepdim=True)
                temp_dis = torch.matmul(torch.matmul(diff.T, covariance_matrix), diff)
                mea_sim[0, j * h * w:(j + 1) * h * w] = temp_dis.diag()

            mahalanobis.append(mea_sim.view(1, -1))

        mahalanobis = torch.cat(mahalanobis, 0)
        # self.conv1d=nn.Conv1d(in_channels, out_channels, kernel_size, stride)
        mahalanobis=self.conv1d(mahalanobis)

        return mahalanobis


    def forward(self, x1, x2):

        CovaMatrix_list = self.cal_covariance(x2)
        maha_sim = self.mahalanobis_similarity(x1, CovaMatrix_list)

        return maha_sim

class Conv_first(nn.Module):
    def __init__(self, features):
        super(Conv_first, self).__init__()
        self.conv_first= nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]
            nn.LeakyReLU(0.2, True)
        )
    def forward(self, x):
        x = self.conv_first(x)
        return x

import torch
from torch import nn
import functools
#Gia su input dau vao co dang la [C, H, W]= [batch_size, 1,64,64]
class SAFE(nn.Module):
    def __init__(self, features, M=3, stride=1):
        super(SAFE, self).__init__()
        d = features
        norm_layer = nn.BatchNorm2d
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        # self.conv1 = nn.Sequential(
        #       nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]
        #       nn.LeakyReLU(0.2, True),
        #       nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]
        #       nn.LeakyReLU(0.2, True)
        #   )
        self.M = M
        self.features = features
        self.convs = nn.ModuleList([])
        for i in range(M):
            self.convs.append(nn.Sequential(
                nn.Conv2d(features, features, kernel_size=3, stride=stride, padding=1 + i, dilation=1 + i,
                          groups=features, bias=False),
                nn.BatchNorm2d(features),
                nn.ReLU(inplace=True)
            )) #Xay dung M nhanh tich chap voi cac gia tri dilation thay doi tu 1->3
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(nn.Conv2d(features, d, kernel_size=1, stride=1, bias=False), #FC
                                nn.LayerNorm([features, 1, 1]),
                                nn.ReLU(inplace=True))
        self.fcs = nn.ModuleList([])
        for i in range(M):
            self.fcs.append(
                nn.Conv2d(d, features, kernel_size=1, stride=1) #FC
            )
        self.softmax = nn.Softmax(dim=1) #Softmax theo chieu cua

    def forward(self, x):
        # x = self.conv1(x) #Di qua lop conv trich xuat dac trung (tang channel) #[batch_size, 64,16,16]
        batch_size = x.shape[0]

        feats = [conv(x) for conv in self.convs] #Voi dau ra moi output la (batch_size,64,16,16)
        feats = torch.cat(feats, dim=1) #Output la [batch_size, 64x64x64, 16, 16]=[batch_size, 192, 16, 16]
        feats = feats.view(batch_size, self.M, self.features, feats.shape[2], feats.shape[3])  #Reshape [batch_size, M, 64, 16, 16]

        feats_U = torch.sum(feats, dim=1) #Tong voi dim=1 (Hay co the noi la dim cua M)
        feats_S = self.gap(feats_U)  #Dau ra co dang la [batch_Size, 64, 1, 1]
        feats_Z = self.fc(feats_S)  #Anh xa qua mien d [batch_size, d, 1, 1]

        mask = [fc(feats_Z) for fc in self.fcs] #Chung ta se co 3 output voi moi output la [batch_size, 64, 1, 1]
        mask = torch.cat(mask, dim=1) #(batch_size,M*features,1,1).
        mask = mask.view(batch_size, self.M, self.features, 1, 1) #Kha la chuan  so voi minh nghi
        mask = self.softmax(mask) #Softmax ben tren la voi dim=1 (chieu cua M) [batch_size, 64, 1, 1]
        feats_V = torch.sum(feats * mask, dim=1) #[batch_size, 64, 16, 16]
        #Tung gia tri [1,1] o mask se nhan voi  ca khong gian [H, W]

        return feats_V

class Block_1(nn.Module):
  def __init__ (self,dim=64):
    super(Block_1,self).__init__()
    self.dim=dim
    self.conv_first=Conv_first(self.dim)
    # self.Mamba_block=Mamba_Block(self.dim)
    self.SAFE=SAFE(self.dim)
    self.Encoder_Decoder=Encoder_Decoder(self.dim)

    # self.covariance=MahalanobisBlock(1,self.dim,64,64)
  def forward (self,q,S):
    q=self.conv_first(q)
    q=self.SAFE(q)
    feature=[]
    for i in S:
      i=self.conv_first(i)
      i=self.SAFE(i)
      feature.append(i)
    S=torch.stack(feature,dim=0)
    output=self.Encoder_Decoder(q,S)
    return output, q,S

class Block_2(nn.Module):
  def __init__ (self,dim=64):
    super(Block_2,self).__init__()
    self.dim=dim
    # self.conv_first=Conv_first(self.dim)
    self.SS_Conv_SSM=SS_Conv_SSM(self.dim)
    # self.Encoder_Decoder=Encoder_Decoder(self.dim)
    self.covariance=MahalanobisBlock(1,self.dim,64,64)

  def forward (self,q,S):
    # q=self.conv_first(q)
    q=self.SS_Conv_SSM(q)
    feature=[]
    for i in S:
      # i=self.conv_first(i)
      i=self.SS_Conv_SSM(i)
      feature.append(i)
    S=torch.stack(feature,dim=0)
    output=self.covariance(q,S)
    return output, q,S

class Model(nn.Module):
  def __init__ (self,dim=64,num_class=13):
    super(Model, self).__init__()
    self.dim=dim
    self.Block_1=Block_1(self.dim) #self.conv first
    self.Block_2=Block_2(self.dim) # SS_Conv_SSM
    self.w1 = nn.Parameter(torch.tensor(1.0))
    self.w2 = nn.Parameter(torch.tensor(1.0))
    self.num_class=num_class
    self.conv1d=nn.Conv1d(num_class,num_class,dim)

  def forward (self,q,S):
    output_1,q,S=self.Block_1(q,S)
    output_2,q,S=self.Block_2(q,S)
    output2=self.w1*output_2 # [B,num_class, 64]
    output1=self.w2*output_1
    output1=self.conv1d(output1)
    output1=output1.squeeze(dim=2)
    output=output1+output2


    return output
