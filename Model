{"cells":[{"cell_type":"markdown","metadata":{"id":"lug0l3Y9z_7T"},"source":["\n","# ***1. PREPARING LIBRARIES***\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M6N2TryW4_Ea"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3571,"status":"ok","timestamp":1754210415172,"user":{"displayName":"Hồng Anh Đặng","userId":"02551723116795104462"},"user_tz":-420},"id":"zrBm_gFaz3T-","outputId":"e70d615b-43bd-4e8f-98a2-fa428bfe29fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRkzuZpO0bdK"},"outputs":[],"source":["!pip install torch==2.4.0 torchvision==0.19.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15002,"status":"ok","timestamp":1754210430155,"user":{"displayName":"Hồng Anh Đặng","userId":"02551723116795104462"},"user_tz":-420},"id":"Bz-VxciP3WAe","outputId":"69bc3de9-5c45-4f36-fa9f-b7b71017d052"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: mamba-ssm==2.2.4 in /usr/local/lib/python3.11/dist-packages (2.2.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (2.6.0+cu124)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (1.11.1.4)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (0.8.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (4.54.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (25.0)\n","Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4) (75.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm==2.2.4) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (0.34.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (2.0.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm==2.2.4) (1.1.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm==2.2.4) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4) (2025.7.14)\n"]}],"source":["!pip install mamba-ssm==2.2.4 --no-build-isolation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22584,"status":"ok","timestamp":1754210452731,"user":{"displayName":"Hồng Anh Đặng","userId":"02551723116795104462"},"user_tz":-420},"id":"hG17Lh1j0fmj","outputId":"2615c8bf-fa1d-4176-b419-7af719afda8d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"]}],"source":["''' Torch '''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","''' Mamba '''\n","from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n","from functools import partial\n","from typing import Optional, Callable\n","from timm.models.layers import DropPath\n","import math\n","\n","''' Scikit '''\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.utils import shuffle\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import confusion_matrix\n","\n","''' Scipy '''\n","import scipy.io\n","import scipy.io as sio\n","from scipy.ndimage import gaussian_filter\n","from scipy.io import loadmat\n","\n","''' Einops '''\n","from einops import rearrange, repeat\n","from einops import reduce\n","\n","''' Other '''\n","from random import randint\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import clear_output\n","import pandas as pd\n","import os,re\n","import errno\n","import random\n","import urllib.request as urllib\n","import librosa\n","import numpy as np\n","import cv2\n","import functools\n","from tqdm import tqdm\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"x4eIeTqZ4lCV"},"source":["# ***5. MODEL***"]},{"cell_type":"markdown","metadata":{"id":"tpiHZyOCKUVF"},"source":["## PCA-PSA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GLh3Dniy7oeI"},"outputs":[],"source":["from einops import rearrange\n","\n","class PCA(nn.Module):\n","  def __init__(self, dim,kernel_size):\n","    super().__init__()\n","    self.dw = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=\"same\", groups=dim)\n","    self.prob = nn.Softmax(dim=1)\n","\n","  def forward(self,x):\n","    c = reduce(x, 'b c h w -> b c', 'mean')\n","    x = self.dw(x)\n","    c_ = reduce(x, 'b c h w -> b c', 'mean')\n","    raise_ch = self.prob(c_ - c)\n","    att_score = torch.sigmoid(c_ + c_*raise_ch)\n","    return torch.einsum('bchw, bc -> bchw', x, att_score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"minoJXBDwzwt"},"outputs":[],"source":["class PSA(nn.Module):\n","  def __init__(self, dim,kernel_size):\n","    super().__init__()\n","    self.pw = nn.Conv2d(dim, dim, kernel_size=1)\n","    self.prob = nn.Softmax2d()\n","\n","  def forward(self,x):\n","    s = reduce(x, 'b c w h -> b w h', 'mean')\n","    xp = self.pw(x)\n","    s_ = reduce(xp, 'b c w h -> b w h', 'mean')\n","    raise_sp = self.prob(s_ - s)\n","    att_score = torch.sigmoid(s_ + s_*raise_sp)\n","    return torch.einsum('bchw, bwh -> bchw', x, att_score)"]},{"cell_type":"markdown","metadata":{"id":"Vtj3fTiLKYBr"},"source":["## LKA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zb4KxT-Y7obr"},"outputs":[],"source":["class LKA(nn.Module):\n","    def __init__(self, dim, kernel_size, dilated_rate=3):\n","        super().__init__()\n","        #self.conv0 = nn.Conv2d(dim, dim, kernel_size, padding='same', groups=dim)\n","        self.conv0 = PCA(dim,kernel_size=kernel_size)\n","        self.conv_spatial = nn.Conv2d(dim, dim, kernel_size=7, stride=1, padding='same', groups=dim, dilation=dilated_rate)\n","        #self.conv1 = nn.Conv2d(dim, dim, 1)\n","        self.conv1 = PSA(dim,kernel_size=kernel_size)\n","        self.norm = nn.BatchNorm2d(dim)\n","    def forward(self, x):\n","        u = x.clone()\n","        attn = self.conv0(x)\n","        attn = self.conv_spatial(attn)\n","        # attn = F.sigmoid(self.conv1(attn))\n","        return u*attn"]},{"cell_type":"markdown","metadata":{"id":"6vOuxu6XKaFg"},"source":["## Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"P212bFwI7oVm"},"outputs":[],"source":["class my_norm(nn.Module):\n","    def __init__(self, shape=4096):\n","        super().__init__()\n","        self.shape = shape\n","        self.norm = nn.LayerNorm(shape)\n","    def forward(self, x):\n","        B,C,H,W = x.shape\n","        x = x.view(B,C,-1)\n","        x = self.norm(x)\n","        x = x.view(B,C,H,W)\n","        return x\n","\n","class MultiScaleExtractor(nn.Module):\n","    def __init__(self, dim=64):\n","        super().__init__()\n","        # self.head_pw = nn.Conv2d(dim, dim, 1)\n","        self.tail_pw = nn.Conv2d(dim, dim, 1)\n","\n","        self.LKA3 = LKA(dim, kernel_size=3)\n","        self.LKA5 = LKA(dim, kernel_size=5)\n","        self.LKA7 = LKA(dim, kernel_size=7)\n","        self.norm3 = nn.BatchNorm2d(dim)\n","        self.norm5 = nn.BatchNorm2d(dim)\n","        self.norm7 = nn.BatchNorm2d(dim)\n","\n","        self.pointwise = nn.Conv2d(dim, dim, 1)\n","        self.conv_cn = nn.Conv2d(dim, dim, 3, groups=dim,padding=1)\n","        self.norm_last = nn.BatchNorm2d(dim)\n","    def forward(self, x):\n","        x_copy = x.clone()\n","        # x = self.head_pw(x)\n","\n","        x3 = self.LKA3(x) + x\n","        x3 = self.norm3(x3)\n","        x5 = self.LKA5(x) + x\n","        x5 = self.norm5(x5)\n","        x7 = self.LKA7(x) + x\n","        x7 = self.norm7(x7)\n","\n","        x = F.gelu(x3 + x5 + x7)\n","        x = self.tail_pw(x) + x_copy\n","\n","        x = self.pointwise(x)\n","        x = self.conv_cn(x)\n","        x = F.gelu(self.norm_last(x))\n","        return x\n","\n","def Feature_Extractor(dim=64, patch_size=4, depth=2):\n","    return nn.Sequential(\n","      *[MultiScaleExtractor(dim=dim) for _ in range(depth)]\n","    )"]},{"cell_type":"markdown","metadata":{"id":"w68qBYTkDnsC"},"source":["## Mamba ##\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lBsrgSsrDty-"},"outputs":[],"source":["import time\n","import math\n","from functools import partial\n","from typing import Optional, Callable\n","from torch import Tensor\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from einops import rearrange, repeat\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","try:\n","    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n","except:\n","    pass\n","\n","# an alternative for mamba_ssm (in which causal_conv1d is needed)\n","try:\n","    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n","    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n","except:\n","    pass\n","\n","DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n","\n","\n","def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n","    \"\"\"\n","    u: r(B D L)\n","    delta: r(B D L)\n","    A: r(D N)\n","    B: r(B N L)\n","    C: r(B N L)\n","    D: r(D)\n","    z: r(B D L)\n","    delta_bias: r(D), fp32\n","\n","    ignores:\n","        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n","    \"\"\"\n","    import numpy as np\n","\n","    # fvcore.nn.jit_handles\n","    def get_flops_einsum(input_shapes, equation):\n","        np_arrs = [np.zeros(s) for s in input_shapes]\n","        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n","        for line in optim.split(\"\\n\"):\n","            if \"optimized flop\" in line.lower():\n","                # divided by 2 because we count MAC (multiply-add counted as one flop)\n","                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n","                return flop\n","\n","\n","    assert not with_complex\n","\n","    flops = 0 # below code flops = 0\n","    if False:\n","        ...\n","        \"\"\"\n","        dtype_in = u.dtype\n","        u = u.float()\n","        delta = delta.float()\n","        if delta_bias is not None:\n","            delta = delta + delta_bias[..., None].float()\n","        if delta_softplus:\n","            delta = F.softplus(delta)\n","        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n","        is_variable_B = B.dim() >= 3\n","        is_variable_C = C.dim() >= 3\n","        if A.is_complex():\n","            if is_variable_B:\n","                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n","            if is_variable_C:\n","                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n","        else:\n","            B = B.float()\n","            C = C.float()\n","        x = A.new_zeros((batch, dim, dstate))\n","        ys = []\n","        \"\"\"\n","\n","    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n","    if with_Group:\n","        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n","    else:\n","        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n","    if False:\n","        ...\n","        \"\"\"\n","        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n","        if not is_variable_B:\n","            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n","        else:\n","            if B.dim() == 3:\n","                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n","            else:\n","                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n","                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n","        if is_variable_C and C.dim() == 4:\n","            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n","        last_state = None\n","        \"\"\"\n","\n","    in_for_flops = B * D * N\n","    if with_Group:\n","        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n","    else:\n","        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n","    flops += L * in_for_flops\n","    if False:\n","        ...\n","        \"\"\"\n","        for i in range(u.shape[2]):\n","            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n","            if not is_variable_C:\n","                y = torch.einsum('bdn,dn->bd', x, C)\n","            else:\n","                if C.dim() == 3:\n","                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n","                else:\n","                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n","            if i == u.shape[2] - 1:\n","                last_state = x\n","            if y.is_complex():\n","                y = y.real * 2\n","            ys.append(y)\n","        y = torch.stack(ys, dim=2) # (batch dim L)\n","        \"\"\"\n","\n","    if with_D:\n","        flops += B * D * L\n","    if with_Z:\n","        flops += B * D * L\n","    if False:\n","        ...\n","        \"\"\"\n","        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n","        if z is not None:\n","            out = out * F.silu(z)\n","        out = out.to(dtype=dtype_in)\n","        \"\"\"\n","\n","    return flops\n","\n","\n","class PatchEmbed2D(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","    Args:\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n","        super().__init__()\n","        if isinstance(patch_size, int):\n","            patch_size = (patch_size, patch_size)\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        x = self.proj(x).permute(0, 2, 3, 1)\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","\n","class PatchMerging2D(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        B, H, W, C = x.shape\n","\n","        SHAPE_FIX = [-1, -1]\n","        if (W % 2 != 0) or (H % 2 != 0):\n","            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n","            SHAPE_FIX[0] = H // 2\n","            SHAPE_FIX[1] = W // 2\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","\n","        if SHAPE_FIX[0] > 0:\n","            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n","            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n","            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n","            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n","\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","\n","class PatchExpand2D(nn.Module):\n","    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim*2\n","        self.dim_scale = dim_scale\n","        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)\n","        self.norm = norm_layer(self.dim // dim_scale)\n","\n","    def forward(self, x):\n","        B, H, W, C = x.shape\n","        x = self.expand(x)\n","\n","        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)\n","        x= self.norm(x)\n","\n","        return x\n","\n","\n","class Final_PatchExpand2D(nn.Module):\n","    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.dim_scale = dim_scale\n","        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)\n","        self.norm = norm_layer(self.dim // dim_scale)\n","\n","    def forward(self, x):\n","        B, H, W, C = x.shape\n","        x = self.expand(x)\n","\n","        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)\n","        x= self.norm(x)\n","\n","        return x\n","\n","\n","class SS2D(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,\n","        d_state=16,\n","        # d_state=\"auto\", # 20240109\n","        d_conv=3,\n","        expand=2,\n","        dt_rank=\"auto\",\n","        dt_min=0.001,\n","        dt_max=0.1,\n","        dt_init=\"random\",\n","        dt_scale=1.0,\n","        dt_init_floor=1e-4,\n","        dropout=0.,\n","        conv_bias=True,\n","        bias=False,\n","        device=None,\n","        dtype=None,\n","        **kwargs,\n","    ):\n","        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n","        super().__init__()\n","        self.d_model = d_model\n","        self.d_state = d_state\n","        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n","        self.d_conv = d_conv\n","        self.expand = expand\n","        self.d_inner = int(self.expand * self.d_model)\n","        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n","\n","        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n","        self.conv2d = nn.Conv2d(\n","            in_channels=self.d_inner,\n","            out_channels=self.d_inner,\n","            groups=self.d_inner,\n","            bias=conv_bias,\n","            kernel_size=d_conv,\n","            padding=(d_conv - 1) // 2,\n","            **factory_kwargs,\n","        )\n","        self.act = nn.SiLU()\n","\n","        self.x_proj = (\n","            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n","            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n","            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n","            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n","        )\n","        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n","        del self.x_proj\n","\n","        self.dt_projs = (\n","            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n","            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n","            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n","            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n","        )\n","        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n","        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n","        del self.dt_projs\n","\n","        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n","        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n","\n","        # self.selective_scan = selective_scan_fn\n","        self.forward_core = self.forward_corev0\n","\n","        self.out_norm = nn.LayerNorm(self.d_inner)\n","        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n","        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n","\n","    @staticmethod\n","    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n","        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n","\n","        # Initialize special dt projection to preserve variance at initialization\n","        dt_init_std = dt_rank**-0.5 * dt_scale\n","        if dt_init == \"constant\":\n","            nn.init.constant_(dt_proj.weight, dt_init_std)\n","        elif dt_init == \"random\":\n","            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n","        else:\n","            raise NotImplementedError\n","\n","        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n","        dt = torch.exp(\n","            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n","            + math.log(dt_min)\n","        ).clamp(min=dt_init_floor)\n","        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n","        inv_dt = dt + torch.log(-torch.expm1(-dt))\n","        with torch.no_grad():\n","            dt_proj.bias.copy_(inv_dt)\n","        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n","        dt_proj.bias._no_reinit = True\n","\n","        return dt_proj\n","\n","    @staticmethod\n","    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n","        # S4D real initialization\n","        A = repeat(\n","            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n","            \"n -> d n\",\n","            d=d_inner,\n","        ).contiguous()\n","        A_log = torch.log(A)  # Keep A_log in fp32\n","        if copies > 1:\n","            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n","            if merge:\n","                A_log = A_log.flatten(0, 1)\n","        A_log = nn.Parameter(A_log)\n","        A_log._no_weight_decay = True\n","        return A_log\n","\n","    @staticmethod\n","    def D_init(d_inner, copies=1, device=None, merge=True):\n","        # D \"skip\" parameter\n","        D = torch.ones(d_inner, device=device)\n","        if copies > 1:\n","            D = repeat(D, \"n1 -> r n1\", r=copies)\n","            if merge:\n","                D = D.flatten(0, 1)\n","        D = nn.Parameter(D)  # Keep in fp32\n","        D._no_weight_decay = True\n","        return D\n","\n","    def forward_corev0(self, x: torch.Tensor):\n","        self.selective_scan = selective_scan_fn\n","\n","        B, C, H, W = x.shape\n","        L = H * W\n","        K = 4\n","\n","        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n","        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n","\n","        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n","        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n","        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n","        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n","        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n","\n","        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n","        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n","        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n","        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n","        Ds = self.Ds.float().view(-1) # (k * d)\n","        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n","        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n","\n","        out_y = self.selective_scan(\n","            xs, dts,\n","            As, Bs, Cs, Ds, z=None,\n","            delta_bias=dt_projs_bias,\n","            delta_softplus=True,\n","            return_last_state=False,\n","        ).view(B, K, -1, L)\n","        assert out_y.dtype == torch.float\n","\n","        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n","        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n","        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n","\n","        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n","\n","    # an alternative to forward_corev1\n","    def forward_corev1(self, x: torch.Tensor):\n","        self.selective_scan = selective_scan_fn_v1\n","\n","        B, C, H, W = x.shape\n","        L = H * W\n","        K = 4\n","\n","        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n","        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n","\n","        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n","        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n","        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n","        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n","        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n","\n","        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n","        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n","        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n","        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n","        Ds = self.Ds.float().view(-1) # (k * d)\n","        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n","        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n","\n","        out_y = self.selective_scan(\n","            xs, dts,\n","            As, Bs, Cs, Ds,\n","            delta_bias=dt_projs_bias,\n","            delta_softplus=True,\n","        ).view(B, K, -1, L)\n","        assert out_y.dtype == torch.float\n","\n","        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n","        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n","        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n","\n","        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n","\n","    def forward(self, x: torch.Tensor, **kwargs):\n","        B, H, W, C = x.shape\n","\n","        xz = self.in_proj(x)\n","        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n","\n","        x = x.permute(0, 3, 1, 2).contiguous()\n","        x = self.act(self.conv2d(x)) # (b, d, h, w)\n","        y1, y2, y3, y4 = self.forward_core(x)\n","        assert y1.dtype == torch.float32\n","        y = y1 + y2 + y3 + y4\n","        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n","        y = self.out_norm(y)\n","        y = y * F.silu(z)\n","        out = self.out_proj(y)\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","        return out\n","\n","\n","def channel_shuffle(x: Tensor, groups: int) -> Tensor:\n","\n","    batch_size, height, width, num_channels = x.size()\n","    channels_per_group = num_channels // groups\n","\n","    # reshape\n","    # [batch_size, num_channels, height, width] -> [batch_size, groups, channels_per_group, height, width]\n","    x = x.view(batch_size, height, width, groups, channels_per_group)\n","\n","    x = torch.transpose(x, 3, 4).contiguous()\n","\n","    # flatten\n","    x = x.view(batch_size, height, width, -1)\n","\n","    return x\n","\n","class SS_Conv_SSM(nn.Module):\n","    def __init__(\n","        self,\n","        hidden_dim: int = 0,\n","        drop_path: float = 0,\n","        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n","        attn_drop_rate: float = 0,\n","        d_state: int = 16,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.ln_1 = norm_layer(hidden_dim//2)\n","        self.self_attention = SS2D(d_model=hidden_dim//2, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n","        self.drop_path = DropPath(drop_path)\n","\n","        self.Feature_Extractor = Feature_Extractor(32)\n","        # self.finalconv11 = nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, stride=1)\n","    def forward(self, input: torch.Tensor):\n","        ori=input\n","\n","        input_left, input_right = input.chunk(2,dim=1)\n","        input_right = self.drop_path(self.self_attention(self.ln_1(input_right.permute(0,2,3,1))))\n","        input_right = input_right.permute(0,3,1,2).contiguous()\n","\n","        input_left = self.Feature_Extractor ( input_left)\n","\n","        output = torch.cat((input_left,input_right),dim=1)\n","        output = channel_shuffle(output,groups=2)\n","        return output+ori"]},{"cell_type":"markdown","metadata":{"id":"4lpt9_dcKrp9"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"o92X0T98DXmR"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, dim):\n","        super(Encoder, self).__init__()\n","        self.dim = dim\n","        self.GAP = nn.AdaptiveAvgPool2d((1, 1))\n","        self.ScaledDotProductAttention = ScaledDotProductAttention()\n","        self.norm = nn.LayerNorm(normalized_shape=self.dim)\n","        self.FFN = nn.Sequential(\n","            nn.Linear(self.dim, self.dim),\n","            nn.GELU(),\n","            nn.Linear(self.dim, self.dim),\n","            nn.GELU()\n","        )\n","\n","    def forward(self, Support):\n","      encoded = []\n","      for index in range(len(Support)):\n","          s = Support[index]                                  # [B, C, H, W]\n","          s = self.GAP(s)                                     # [B, C, 1, 1]\n","          s = s.view(s.size(0), s.size(1))                    # [B, C]\n","          s = self.ScaledDotProductAttention(s, s, s) + s     # [B, C]\n","          s = self.norm(s)                                    # [B, C]\n","          s = self.FFN(s) + s                                 # [B, C]\n","          s = self.norm(s)                                    # [B, C]\n","          s = torch.mean(s, dim=0, keepdim=True)\n","          encoded.append(s)\n","\n","      return encoded                                         # [Num_class x (B, C)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"reh9gw-oAnBb"},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, dim=64):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.q_linear = nn.Linear(dim, dim)\n","        self.k_linear = nn.Linear(dim, dim)\n","        self.v_linear = nn.Linear(dim, dim)\n","        self.dim = dim\n","\n","    def forward(self, q, k, v):\n","        \"\"\"\n","        Args:\n","            q (Tensor): Query tensor of shape (batch_size, dim).\n","            k (Tensor): Key tensor of shape (batch_size, dim).\n","            v (Tensor): Value tensor of shape (batch_size, dim).\n","\n","        Returns:\n","            output (Tensor): Scaled Dot-Product Attention output tensor of shape (batch_size, dim).\n","        \"\"\"\n","        q = self.q_linear(q)\n","        k = self.k_linear(k)\n","        v = self.v_linear(v)\n","\n","        scaled_dot_product = torch.matmul(q.unsqueeze(2), k.unsqueeze(1)) / torch.sqrt(torch.tensor(self.dim, dtype=torch.float32))\n","        attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n","        output = torch.matmul(attention_weights, v.unsqueeze(2))\n","        output = output.squeeze(2)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MPX4sqg5At-S"},"outputs":[],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, dim=64):\n","        super(CrossAttention, self).__init__()\n","        self.q_linear = nn.Linear(dim, dim)\n","        self.k_linear = nn.Linear(dim, dim)\n","        self.v_linear = nn.Linear(dim, dim)\n","        self.dim = dim\n","    def forward(self, q, k, v):\n","        \"\"\"\n","        Args:\n","            q (Tensor): Query tensor of shape (batch_size,  dim).\n","            k (Tensor): Key tensor of shape (batch_size, dim).\n","            v (Tensor): Value tensor of shape (batch_size, dim).\n","\n","        Returns:\n","            output (Tensor): Scaled Dot-Product Attention output tensor of shape (batch_size, dim).\n","        \"\"\"\n","        q = self.q_linear(q)\n","        k = self.k_linear(k)\n","\n","\n","\n","        v = self.v_linear(v)\n","\n","        scaled_dot_product = torch.matmul(q.unsqueeze(2), k.unsqueeze(1))\n","        attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n","        # output = torch.matmul(attention_weights, v)\n","\n","        return attention_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4F4jcOfWAvi5"},"outputs":[],"source":["class Encoder_Decoder(nn.Module):\n","    def __init__(self, dim):\n","        super(Encoder_Decoder, self).__init__()\n","        self.dim = dim\n","        self.encoder_out = Encoder(self.dim)\n","        self.attention = CrossAttention()\n","        self.ScaledDotProductAttention = ScaledDotProductAttention()\n","        self.norm = nn.LayerNorm(normalized_shape=self.dim)\n","        self.GAP = nn.AdaptiveAvgPool2d((1, 1))\n","        self.Linear = nn.Conv1d(in_channels=dim,out_channels=dim,kernel_size=dim)\n","    def forward(self, q, S):\n","        q = self.GAP(q)                                       # [B, C, 1, 1]\n","        q = q.view(q.size(0), q.size(1))                      # [B, C]\n","        q_first = q                                           # [B, C]\n","        q = self.ScaledDotProductAttention(q, q, q) + q_first # [B, C]\n","        q = self.norm(q)                                      # [B, C]\n","        output = []\n","        encoder_outs = self.encoder_out(S)                    # [Num_class x (B, C)]\n","\n","        for encoder_out in encoder_outs:\n","            out = self.attention(q, encoder_out, encoder_out) # [B, C, C]\n","            out = self.Linear(out)                            # [B,C,1]\n","            out = out.squeeze(2)                              # [B,C]\n","            output.append(out)\n","        output=torch.stack(output,dim=1)\n","\n","\n","        return output                                         # [B,Num_class, C)]"]},{"cell_type":"markdown","metadata":{"id":"vyMcM8PStMRS"},"source":["## Mahalanobis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fu8yWlMjtTo2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import functools\n","\n","\n","class MahalanobisBlock(nn.Module):\n","    def __init__(self,B,C,H,W):\n","        super(MahalanobisBlock, self).__init__()\n","        self.B=B\n","        self.C=C\n","        self.H=H\n","        self.W=W\n","        self.conv1d=nn.Conv1d(self.B,self.B,self.H*self.W,self.H*self.W)\n","\n","    def cal_covariance(self, input):\n","        CovaMatrix_list = []\n","        for i in range(len(input)):\n","            support_set_sam = input[i]\n","            B, C, h, w = support_set_sam.size()\n","            local_feature_list = []\n","\n","            for local_feature in support_set_sam:\n","                local_feature_np = local_feature.detach().cpu().numpy()\n","                transposed_tensor = np.transpose(local_feature_np, (1, 2, 0))\n","                reshaped_tensor = np.reshape(transposed_tensor, (h * w, C))\n","\n","                for line in reshaped_tensor:\n","                    local_feature_list.append(line)\n","\n","            local_feature_np = np.array(local_feature_list)\n","            # mean = np.mean(local_feature_np, axis=0)\n","            # local_feature_list = [x - mean for x in local_feature_list]\n","\n","            covariance_matrix = np.cov(local_feature_np, rowvar=False)\n","            covariance_matrix = torch.from_numpy(covariance_matrix)\n","            CovaMatrix_list.append(covariance_matrix)\n","\n","        return CovaMatrix_list # [numclass*[C,C]]\n","\n","\n","\n","    def mahalanobis_similarity(self, input, CovaMatrix_list):\n","        B, C, h, w = input.size()\n","        mahalanobis = []\n","        in_channels=B\n","        out_channels=B\n","        kernel_size=h*w\n","        stride=h*w\n","\n","        for i in range(B):\n","            query_sam = input[i]\n","            query_sam = query_sam.view(C, -1)\n","            query_sam_norm = torch.norm(query_sam, 2, 1, True)\n","            query_sam = query_sam / query_sam_norm\n","            mea_sim = torch.zeros(1, len(CovaMatrix_list) * h * w).cuda()\n","            for j in range(len(CovaMatrix_list)):\n","                covariance_matrix = CovaMatrix_list[j].float().cuda()\n","                diff = query_sam - torch.mean(query_sam, dim=1, keepdim=True)\n","                temp_dis = torch.matmul(torch.matmul(diff.T, covariance_matrix), diff)\n","                mea_sim[0, j * h * w:(j + 1) * h * w] = temp_dis.diag()\n","\n","            mahalanobis.append(mea_sim.view(1, -1))\n","\n","        mahalanobis = torch.cat(mahalanobis, 0)\n","        # self.conv1d=nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n","        mahalanobis=self.conv1d(mahalanobis)\n","\n","        return mahalanobis\n","\n","\n","    def forward(self, x1, x2):\n","\n","        CovaMatrix_list = self.cal_covariance(x2)\n","        maha_sim = self.mahalanobis_similarity(x1, CovaMatrix_list)\n","\n","        return maha_sim"]},{"cell_type":"markdown","metadata":{"id":"BdTq6UHMBEMb"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fDKjeAGrBE9s"},"outputs":[],"source":["class Conv_first(nn.Module):\n","    def __init__(self, features):\n","        super(Conv_first, self).__init__()\n","        self.conv_first= nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]\n","            nn.LeakyReLU(0.2, True),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]\n","            nn.LeakyReLU(0.2, True)\n","        )\n","    def forward(self, x):\n","        x = self.conv_first(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UV9CGhQDBGFv"},"outputs":[],"source":["import torch\n","from torch import nn\n","import functools\n","#Gia su input dau vao co dang la [C, H, W]= [batch_size, 1,64,64]\n","class SAFE(nn.Module):\n","    def __init__(self, features, M=3, stride=1):\n","        super(SAFE, self).__init__()\n","        d = features\n","        norm_layer = nn.BatchNorm2d\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        # self.conv1 = nn.Sequential(\n","        #       nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]\n","        #       nn.LeakyReLU(0.2, True),\n","        #       nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #[batch_size, 64,64,64]\n","        #       nn.LeakyReLU(0.2, True)\n","        #   )\n","        self.M = M\n","        self.features = features\n","        self.convs = nn.ModuleList([])\n","        for i in range(M):\n","            self.convs.append(nn.Sequential(\n","                nn.Conv2d(features, features, kernel_size=3, stride=stride, padding=1 + i, dilation=1 + i,\n","                          groups=features, bias=False),\n","                nn.BatchNorm2d(features),\n","                nn.ReLU(inplace=True)\n","            )) #Xay dung M nhanh tich chap voi cac gia tri dilation thay doi tu 1->3\n","        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Sequential(nn.Conv2d(features, d, kernel_size=1, stride=1, bias=False), #FC\n","                                nn.LayerNorm([features, 1, 1]),\n","                                nn.ReLU(inplace=True))\n","        self.fcs = nn.ModuleList([])\n","        for i in range(M):\n","            self.fcs.append(\n","                nn.Conv2d(d, features, kernel_size=1, stride=1) #FC\n","            )\n","        self.softmax = nn.Softmax(dim=1) #Softmax theo chieu cua\n","\n","    def forward(self, x):\n","        # x = self.conv1(x) #Di qua lop conv trich xuat dac trung (tang channel) #[batch_size, 64,16,16]\n","        batch_size = x.shape[0]\n","\n","        feats = [conv(x) for conv in self.convs] #Voi dau ra moi output la (batch_size,64,16,16)\n","        feats = torch.cat(feats, dim=1) #Output la [batch_size, 64x64x64, 16, 16]=[batch_size, 192, 16, 16]\n","        feats = feats.view(batch_size, self.M, self.features, feats.shape[2], feats.shape[3])  #Reshape [batch_size, M, 64, 16, 16]\n","\n","        feats_U = torch.sum(feats, dim=1) #Tong voi dim=1 (Hay co the noi la dim cua M)\n","        feats_S = self.gap(feats_U)  #Dau ra co dang la [batch_Size, 64, 1, 1]\n","        feats_Z = self.fc(feats_S)  #Anh xa qua mien d [batch_size, d, 1, 1]\n","\n","        mask = [fc(feats_Z) for fc in self.fcs] #Chung ta se co 3 output voi moi output la [batch_size, 64, 1, 1]\n","        mask = torch.cat(mask, dim=1) #(batch_size,M*features,1,1).\n","        mask = mask.view(batch_size, self.M, self.features, 1, 1) #Kha la chuan  so voi minh nghi\n","        mask = self.softmax(mask) #Softmax ben tren la voi dim=1 (chieu cua M) [batch_size, 64, 1, 1]\n","        feats_V = torch.sum(feats * mask, dim=1) #[batch_size, 64, 16, 16]\n","        #Tung gia tri [1,1] o mask se nhan voi  ca khong gian [H, W]\n","\n","        return feats_V"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ExEq92lsBJaZ"},"outputs":[],"source":["class Block_1(nn.Module):\n","  def __init__ (self,dim=64):\n","    super(Block_1,self).__init__()\n","    self.dim=dim\n","    self.conv_first=Conv_first(self.dim)\n","    # self.Mamba_block=Mamba_Block(self.dim)\n","    self.SAFE=SAFE(self.dim)\n","    self.Encoder_Decoder=Encoder_Decoder(self.dim)\n","\n","    # self.covariance=MahalanobisBlock(1,self.dim,64,64)\n","  def forward (self,q,S):\n","    q=self.conv_first(q)\n","    q=self.SAFE(q)\n","    feature=[]\n","    for i in S:\n","      i=self.conv_first(i)\n","      i=self.SAFE(i)\n","      feature.append(i)\n","    S=torch.stack(feature,dim=0)\n","    output=self.Encoder_Decoder(q,S)\n","    return output, q,S"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ykuhN-WsbZHH"},"outputs":[],"source":["class Block_2(nn.Module):\n","  def __init__ (self,dim=64):\n","    super(Block_2,self).__init__()\n","    self.dim=dim\n","    # self.conv_first=Conv_first(self.dim)\n","    self.SS_Conv_SSM=SS_Conv_SSM(self.dim)\n","    # self.Encoder_Decoder=Encoder_Decoder(self.dim)\n","    self.covariance=MahalanobisBlock(1,self.dim,64,64)\n","\n","  def forward (self,q,S):\n","    # q=self.conv_first(q)\n","    q=self.SS_Conv_SSM(q)\n","    feature=[]\n","    for i in S:\n","      # i=self.conv_first(i)\n","      i=self.SS_Conv_SSM(i)\n","      feature.append(i)\n","    S=torch.stack(feature,dim=0)\n","    output=self.covariance(q,S)\n","    return output, q,S"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hSmFEspNBMEB"},"outputs":[],"source":["class Model(nn.Module):\n","  def __init__ (self,dim=64,num_class=13):\n","    super(Model, self).__init__()\n","    self.dim=dim\n","    self.Block_1=Block_1(self.dim) #self.conv first\n","    self.Block_2=Block_2(self.dim) # SS_Conv_SSM\n","    self.w1 = nn.Parameter(torch.tensor(1.0))\n","    self.w2 = nn.Parameter(torch.tensor(1.0))\n","    self.num_class=num_class\n","    self.conv1d=nn.Conv1d(num_class,num_class,dim)\n","\n","  def forward (self,q,S):\n","    output_1,q,S=self.Block_1(q,S)\n","    output_2,q,S=self.Block_2(q,S)\n","    output2=self.w1*output_2 # [B,num_class, 64]\n","    output1=self.w2*output_1\n","    output1=self.conv1d(output1)\n","    output1=output1.squeeze(dim=2)\n","    output=output1+output2\n","\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c0TcjC8VDb2k","outputId":"aaf133ed-c2b4-4f9f-b644-ae3ce5f24624"},"outputs":[{"data":{"text/plain":["torch.Size([1, 13])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["x=torch.rand(13,1,1,64,64).cuda()\n","y=torch.rand(1,1,64,64).cuda()\n","model=Model(dim=64).cuda()\n","model(y,x).shape"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}